# 04 预训练与指令微调

> 本文档由 Vibe Writing 大模型生成初稿，并结合本仓库实践与个人学习需求进行整理与校订。

## 1. 本篇目标
 
 - 区分预训练（Pretrain）与指令微调（SFT / Instruction Tuning）。
 - 理解「自监督学习」与「监督学习」在大模型中的角色分工。
 - 建立从预训练语料到指令数据集的整体认识。
 
 ## 2. 预训练：让模型学会「语言」
 
 - 目标：预测下一个 token（Causal LM）。
 - 数据特点：大规模、泛化、弱约束。
 - 损失函数与训练过程的直观理解。
 
 ## 3. 指令微调：让模型学会「听懂指令」
 
 - 从「通用文本」到「指令 - 回复」数据。
 - High-quality SFT 数据的特点与构造思路。
 - 为什么少量高质量指令数据就能显著改变模型行为。
 
 ## 4. MiniMind 中的对应实现
 
 - 预训练脚本：`train_pretrain.py`。
 - 全参数 SFT：`train_full_sft.py`。
 - LoRA 微调：`train_lora.py` 与相关配置。
 
 ## 5. 数据与配置的常见坑
 
 - 序列长度、batch size、学习率的基本经验。
 - 过拟合 / 欠拟合的简单判别方式。
 
 ## 6. 延伸阅读与思考
 
 - 推荐阅读：InstructGPT / LIMA 等指令微调论文。
 - 思考题：为什么不能只做指令微调，而完全跳过预训练？
 
