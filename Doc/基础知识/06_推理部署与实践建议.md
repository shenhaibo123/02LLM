# 06 推理、部署与实践建议

> 本文档由 Vibe Writing 大模型生成初稿，并结合本仓库实践与个人学习需求进行整理与校订。

## 1. 本篇目标
 
 - 理解离线推理与在线服务的基本差异。
 - 了解常见推理与部署工具链（vLLM / llama.cpp / OpenAI-API 风格服务等）。
 - 建立从本地实验到小规模线上服务的整体认识。
 
 ## 2. 推理基础概念
 
 - 贪心、采样、top-k、top-p 等解码策略。
 - temperature 等采样超参数的直观含义。
 - 延迟、吞吐量与显存占用的基本权衡。
 
 ## 3. 部署形态举例
 
 - 本地脚本推理：直接加载权重进行生成。
 - HTTP / OpenAI-API 协议服务：作为后端被前端或其他系统调用。
 - 结合 RAG 与向量数据库的应用形态。
 
 ## 4. MiniMind 项目中的实践路径
 
 - `serve_openai_api.py` 与 `chat_openai_api.py` 的角色。
 - `web_demo.py`：基于 streamlit 的简易 Web 聊天界面。
 - 与第三方推理引擎（vLLM / llama.cpp / ollama）的联动方向。
 
 ## 5. 从「玩模型」到「用模型」的注意事项
 
 - 监控与日志：如何发现异常输出与失败请求。
 - 安全与合规的基本思路（过滤、有害内容防护等）。
 - 资源成本估算与简单优化建议。
 
 ## 6. 延伸阅读与思考
 
 - 推荐阅读：各大开源推理框架的部署文档。
 - 思考题：如果显存非常有限，如何在体验与成本之间做平衡？
 
